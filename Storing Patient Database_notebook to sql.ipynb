{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b92ed2a-32a1-4170-9911-d47c8c61125a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gender'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentry\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     11\u001b[0m         patient \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresource\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m         patient_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfullUrl\u001b[39m\u001b[38;5;124m'\u001b[39m: entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfullUrl\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresourceType\u001b[39m\u001b[38;5;124m'\u001b[39m: patient[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresourceType\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     15\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: patient[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 16\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mpatient\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbirthDate\u001b[39m\u001b[38;5;124m'\u001b[39m: patient[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbirthDate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     18\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeceasedDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m: patient\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeceasedDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     19\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaritalStatus\u001b[39m\u001b[38;5;124m'\u001b[39m: patient\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaritalStatus\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     20\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m: patient[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m: patient[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m: patient[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m         }\n\u001b[0;32m     24\u001b[0m         data_list\u001b[38;5;241m.\u001b[39mappend(patient_data)\n\u001b[0;32m     25\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data_list)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gender'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pyodbc\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json']\n",
    "data_list = []\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        patient = entry['resource']\n",
    "        patient_data = {\n",
    "            'fullUrl': entry['fullUrl'],\n",
    "            'resourceType': patient['resourceType'],\n",
    "            'id': patient['id'],\n",
    "            'gender': patient['gender'],\n",
    "            'birthDate': patient['birthDate'],\n",
    "            'deceasedDateTime': patient.get('deceasedDateTime', ''),\n",
    "            'maritalStatus': patient.get('maritalStatus', {}).get('text', ''),\n",
    "            'city': patient['address'][0]['city'],\n",
    "            'state': patient['address'][0]['state'],\n",
    "            'country': patient['address'][0]['country']\n",
    "        }\n",
    "        data_list.append(patient_data)\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv('patient_information.csv', index=False)\n",
    "print(df)\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "username = 'EMIS'\n",
    "password = 'EMIS'\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'UID={username};'\n",
    "    f'PWD={password};'\n",
    "    'Encrypt=no;'\n",
    "    'TrustServerCertificate=yes;'\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "table_name = 'PatientInformation'\n",
    "df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Data has been successfully saved to the SQL Server database '{database}', table '{table_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06557269-cdcd-434b-bd21-cee7b7d3cc9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (878127670.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 25\u001b[1;36m\u001b[0m\n\u001b[1;33m    data_list.append(patient_data)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pyodbc\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json']\n",
    "\n",
    "data_list = []\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        patient = entry['resource']\n",
    "        patient_data = {\n",
    "            'fullUrl': entry['fullUrl'],\n",
    "            'resourceType': patient['resourceType'],\n",
    "            'id': patient['id'],\n",
    "            'gender': patient.get('gender', ''),  # Handle missing gender\n",
    "            'birthDate': patient.get('birthDate', ''),  # Handle missing birthDate\n",
    "            'deceasedDateTime': patient.get('deceasedDateTime', ''),\n",
    "            'maritalStatus': patient.get('maritalStatus', {}).get('text', ''),\n",
    "            'city': patient['address'][0].get('city', '') if 'address' in patient and patient['address'] else '',  # Handle missing address\n",
    "            'state': patient['address'][0].get('state', '') if 'address' in patient and patient['address'] else '',\n",
    "            'country': patient['address'][0].get('country', '') if 'address' in patient and patient['address'] else ''\n",
    "        }\n",
    "         data_list.append(patient_data)\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv('patient_information.csv', index=False)\n",
    "print(df)\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "username = 'EMIS'\n",
    "password = 'EMIS'\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'UID={username};'\n",
    "    f'PWD={password};'\n",
    "    'Encrypt=no;'\n",
    "    'TrustServerCertificate=yes;'\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "table_name = 'PatientInformation'\n",
    "df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Data has been successfully saved to the SQL Server database '{database}', table '{table_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e875368-2642-4191-818f-fdde9359b585",
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "('28000', \"[28000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Login failed for user 'EMIS'. (18456) (SQLDriverConnect); [28000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Login failed for user 'EMIS'. (18456)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 45\u001b[0m\n\u001b[0;32m     34\u001b[0m password \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEMIS\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     36\u001b[0m conn_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDRIVER=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124mODBC Driver 17 for SQL Server\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSERVER=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrustServerCertificate=yes;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     44\u001b[0m )\n\u001b[1;32m---> 45\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mpyodbc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatientInformation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     48\u001b[0m df\u001b[38;5;241m.\u001b[39mto_sql(table_name, conn, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mInterfaceError\u001b[0m: ('28000', \"[28000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Login failed for user 'EMIS'. (18456) (SQLDriverConnect); [28000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Login failed for user 'EMIS'. (18456)\")"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pyodbc\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json']\n",
    "\n",
    "data_list = []\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        patient = entry['resource']\n",
    "        patient_data = {\n",
    "            'fullUrl': entry['fullUrl'],\n",
    "            'resourceType': patient['resourceType'],\n",
    "            'id': patient['id'],\n",
    "            'gender': patient.get('gender', ''),  # Handle missing gender\n",
    "            'birthDate': patient.get('birthDate', ''),  # Handle missing birthDate\n",
    "            'deceasedDateTime': patient.get('deceasedDateTime', ''),\n",
    "            'maritalStatus': patient.get('maritalStatus', {}).get('text', ''),\n",
    "            'city': patient['address'][0].get('city', '') if 'address' in patient and patient['address'] else '',  # Handle missing address\n",
    "            'state': patient['address'][0].get('state', '') if 'address' in patient and patient['address'] else '',\n",
    "            'country': patient['address'][0].get('country', '') if 'address' in patient and patient['address'] else ''\n",
    "        }\n",
    "        data_list.append(patient_data)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv('patient_information.csv', index=False)\n",
    "\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "username = 'EMIS'\n",
    "password = 'EMIS'\n",
    "\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'UID={username};'\n",
    "    f'PWD={password};'\n",
    "    'Encrypt=no;'\n",
    "    'TrustServerCertificate=yes;'\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "table_name = 'PatientInformation'\n",
    "df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Data has been successfully saved to the SQL Server database '{database}', table '{table_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e92837-db94-49fb-b277-e5365de94cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SQL Server using Windows authentication.\n",
      "Error saving data to SQL Server: Execution failed on sql '\n",
      "        SELECT\n",
      "            name\n",
      "        FROM\n",
      "            sqlite_master\n",
      "        WHERE\n",
      "            type IN ('table', 'view')\n",
      "            AND name=?;\n",
      "        ': ('42S02', \"[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid object name 'sqlite_master'. (208) (SQLExecDirectW); [42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Statement(s) could not be prepared. (8180)\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMSAS\\AppData\\Local\\Temp\\ipykernel_26224\\2399485048.py:62: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df.to_sql(table_name, conn, if_exists='replace', index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pyodbc\n",
    "\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json']\n",
    "\n",
    "data_list = []\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        patient = entry['resource']\n",
    "        patient_data = {\n",
    "            'fullUrl': entry['fullUrl'],\n",
    "            'resourceType': patient['resourceType'],\n",
    "            'id': patient['id'],\n",
    "            'gender': patient.get('gender', ''),  # Handle missing gender\n",
    "            'birthDate': patient.get('birthDate', ''),  # Handle missing birthDate\n",
    "            'deceasedDateTime': patient.get('deceasedDateTime', ''),\n",
    "            'maritalStatus': patient.get('maritalStatus', {}).get('text', ''),\n",
    "            'city': patient['address'][0].get('city', '') if 'address' in patient and patient['address'] else '',  # Handle missing address\n",
    "            'state': patient['address'][0].get('state', '') if 'address' in patient and patient['address'] else '',\n",
    "            'country': patient['address'][0].get('country', '') if 'address' in patient and patient['address'] else ''\n",
    "        }\n",
    "        data_list.append(patient_data)\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df.to_csv('patient_information.csv', index=False)\n",
    "\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    'Trusted_Connection=yes;'  # Use Windows authentication\n",
    "    'Encrypt=no;'\n",
    "    'TrustServerCertificate=yes;'\n",
    ")\n",
    "try:\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    print(\"Connected to SQL Server using Windows authentication.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to SQL Server: {e}\")\n",
    "    # Handle the error gracefully, possibly logging it or notifying the user\n",
    "    # You might want to add a 'sys.exit()' here if the connection failure is critical\n",
    "\n",
    "try:\n",
    "    table_name = 'PatientInformation'\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    print(f\"Data has been successfully saved to the SQL Server database '{database}', table '{table_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to SQL Server: {e}\")\n",
    "  \n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d72006-5a67-4b9b-9608-5fcf1bd22fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.30-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: pyodbc in c:\\users\\amsas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\amsas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sqlalchemy) (4.12.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy)\n",
      "  Downloading greenlet-3.0.3-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading SQLAlchemy-2.0.30-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.1 MB 751.6 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.2/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/2.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.6/2.1 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.9/2.1 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.0.3-cp312-cp312-win_amd64.whl (293 kB)\n",
      "   ---------------------------------------- 0.0/293.6 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 194.6/293.6 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 293.6/293.6 kB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.0.3 sqlalchemy-2.0.30\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c117b0-ae75-4f8d-bf3f-0c91d175f230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to the SQL Server database 'EMIS', table 'PatientInformation'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json']\n",
    "\n",
    "data_list = []\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        patient = entry['resource']\n",
    "        patient_data = {\n",
    "            'fullUrl': entry['fullUrl'],\n",
    "            'resourceType': patient['resourceType'],\n",
    "            'id': patient['id'],\n",
    "            'gender': patient.get('gender', ''),  # Handle missing gender\n",
    "            'birthDate': patient.get('birthDate', ''),  # Handle missing birthDate\n",
    "            'deceasedDateTime': patient.get('deceasedDateTime', ''),\n",
    "            'maritalStatus': patient.get('maritalStatus', {}).get('text', ''),\n",
    "            'city': patient['address'][0].get('city', '') if 'address' in patient and patient['address'] else '',  # Handle missing address\n",
    "            'state': patient['address'][0].get('state', '') if 'address' in patient and patient['address'] else '',\n",
    "            'country': patient['address'][0].get('country', '') if 'address' in patient and patient['address'] else ''\n",
    "        }\n",
    "        data_list.append(patient_data)\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df.to_csv('patient_information.csv', index=False)\n",
    "\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "\n",
    "connection_string = f'mssql+pyodbc://{server}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "\n",
    "try:\n",
    "    table_name = 'PatientInformation'\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "    print(f\"Data has been successfully saved to the SQL Server database '{database}', table '{table_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to SQL Server: {e}\")\n",
    "   \n",
    "finally:\n",
    "       engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a4cb8b-76d2-4d14-8bc9-607ad7ab61fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to the SQL Server database 'EMIS', table 'PatientInformation'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json']\n",
    "data_list = []\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        patient = entry['resource']\n",
    "        if patient['resourceType'] == 'Patient':  # Only process Patient resource types\n",
    "            patient_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': patient['resourceType'],\n",
    "                'id': patient['id'],\n",
    "                'gender': patient.get('gender', ''),  # Handle missing gender\n",
    "                'birthDate': patient.get('birthDate', ''),  # Handle missing birthDate\n",
    "                'deceasedDateTime': patient.get('deceasedDateTime', ''),\n",
    "                'maritalStatus': patient.get('maritalStatus', {}).get('text', ''),\n",
    "                'city': patient['address'][0].get('city', '') if 'address' in patient and patient['address'] else '',  # Handle missing address\n",
    "                'state': patient['address'][0].get('state', '') if 'address' in patient and patient['address'] else '',\n",
    "                'country': patient['address'][0].get('country', '') if 'address' in patient and patient['address'] else ''\n",
    "            }\n",
    "            data_list.append(patient_data)\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv('patient_information.csv', index=False)\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "connection_string = f'mssql+pyodbc://{server}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "try:\n",
    "    table_name = 'PatientInformation'\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "    print(f\"Data has been successfully saved to the SQL Server database '{database}', table '{table_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to SQL Server: {e}\")\n",
    "finally:\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f7abea-9a1e-49c0-9d19-9e095f1b5217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving data to SQL Server: (pyodbc.ProgrammingError) ('Invalid parameter type.  param-index=4 param-type=dict', 'HY105')\n",
      "[SQL: INSERT INTO [ResourcesInformation] ([fullUrl], [resourceType], id, [patientId], details) VALUES (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), ( ... 4385 characters truncated ... (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?), (?, ?, ?, ?, ?)]\n",
      "[parameters: ('urn:uuid:23d24349-8f13-e55f-d1d4-e5663aa3f2ad', 'Encounter', '23d24349-8f13-e55f-d1d4-e5663aa3f2ad', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'Encounter', 'id': '23d24349-8f13-e55f-d1d4-e5663aa3f2ad', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefinition/us-c ... (1274 characters truncated) ... reference': 'Organization?identifier=https://github.com/synthetichealth/synthea|c1bf7bfd-7287-37cc-9836-cf77704c49a7', 'display': 'VHM SERVICES INC'}}, 'urn:uuid:8bfe7f7f-435e-87b8-fdfa-bdfad1d9c55b', 'Condition', '8bfe7f7f-435e-87b8-fdfa-bdfad1d9c55b', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'Condition', 'id': '8bfe7f7f-435e-87b8-fdfa-bdfad1d9c55b', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefinition/us-c ... (714 characters truncated) ... ference': 'urn:uuid:23d24349-8f13-e55f-d1d4-e5663aa3f2ad'}, 'onsetDateTime': '1995-07-06T03:00:07+01:00', 'recordedDate': '1995-07-06T03:00:07+01:00'}, 'urn:uuid:0c05b549-291f-4b83-cb24-15d2c3fe3991', 'Condition', '0c05b549-291f-4b83-cb24-15d2c3fe3991', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'Condition', 'id': '0c05b549-291f-4b83-cb24-15d2c3fe3991', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefinition/us-c ... (706 characters truncated) ... ference': 'urn:uuid:23d24349-8f13-e55f-d1d4-e5663aa3f2ad'}, 'onsetDateTime': '1995-07-06T03:58:00+01:00', 'recordedDate': '1995-07-06T03:58:00+01:00'}, 'urn:uuid:c433cb1b-0ffc-d88c-0450-7bf8224992fd', 'Condition', 'c433cb1b-0ffc-d88c-0450-7bf8224992fd', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'Condition', 'id': 'c433cb1b-0ffc-d88c-0450-7bf8224992fd', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefinition/us-c ... (748 characters truncated) ... a3f2ad'}, 'onsetDateTime': '1995-07-06T03:58:00+01:00', 'abatementDateTime': '1996-07-11T03:41:11+01:00', 'recordedDate': '1995-07-06T03:58:00+01:00'}, 'urn:uuid:a8ca5808-114b-377d-7d83-1edbb1519e2f', 'Condition', 'a8ca5808-114b-377d-7d83-1edbb1519e2f', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'Condition', 'id': 'a8ca5808-114b-377d-7d83-1edbb1519e2f', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefinition/us-c ... (719 characters truncated) ... a3f2ad'}, 'onsetDateTime': '1995-07-06T03:58:00+01:00', 'abatementDateTime': '2002-07-18T03:46:42+01:00', 'recordedDate': '1995-07-06T03:58:00+01:00'}, 'urn:uuid:98d96763-9d4c-76bb-97cf-729fbe7d938f', 'DiagnosticReport', '98d96763-9d4c-76bb-97cf-729fbe7d938f', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'DiagnosticReport', 'id': '98d96763-9d4c-76bb-97cf-729fbe7d938f', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefiniti ... (1595 characters truncated) ... ZpbmRpbmcpLCByZWNlaXZlZCBoaWdoZXIgZWR1Y2F0aW9uIChmaW5kaW5nKSwgcGFydC10aW1lIGVtcGxveW1lbnQgKGZpbmRpbmcpLCBzdHJlc3MgKGZpbmRpbmcpLiAKCiMjIFBsYW4KCg=='}]}, 'urn:uuid:2f6e52ab-3a16-486f-e793-7792bbe0b591', 'DocumentReference', '2f6e52ab-3a16-486f-e793-7792bbe0b591', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'DocumentReference', 'id': '2f6e52ab-3a16-486f-e793-7792bbe0b591', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefinit ... (2056 characters truncated) ... 'reference': 'urn:uuid:23d24349-8f13-e55f-d1d4-e5663aa3f2ad'}], 'period': {'start': '1995-07-06T03:00:07+01:00', 'end': '1995-07-06T03:15:07+01:00'}}}, 'urn:uuid:79bbd59f-ddea-ede2-5931-64f817641e26', 'Claim', '79bbd59f-ddea-ede2-5931-64f817641e26', '', {'resourceType': 'Claim', 'id': '79bbd59f-ddea-ede2-5931-64f817641e26', 'status': 'active', 'type': {'coding': [{'system': 'http://terminology.hl7.org ... (2366 characters truncated) ... o/sct', 'code': '73595000', 'display': 'Stress (finding)'}], 'text': 'Stress (finding)'}}], 'total': {'value': 1776.1999999999998, 'currency': 'USD'}}, 'urn:uuid:216cc0b9-6ce5-040d-fd72-bc0b43bb6f75', 'ExplanationOfBenefit', '216cc0b9-6ce5-040d-fd72-bc0b43bb6f75', '', {'resourceType': 'ExplanationOfBenefit', 'id': '216cc0b9-6ce5-040d-fd72-bc0b43bb6f75', 'contained': [{'resourceType': 'ServiceRequest', 'id': 'referra ... (6217 characters truncated) ... , 'text': 'Submitted Amount'}, 'amount': {'value': 1776.1999999999998, 'currency': 'USD'}}], 'payment': {'amount': {'value': 0.0, 'currency': 'USD'}}}, 'urn:uuid:1184edb9-c7bb-0570-80db-074629275bca', 'Encounter', '1184edb9-c7bb-0570-80db-074629275bca', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'Encounter', 'id': '1184edb9-c7bb-0570-80db-074629275bca', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefinition/us-c ... (1274 characters truncated) ... reference': 'Organization?identifier=https://github.com/synthetichealth/synthea|c1bf7bfd-7287-37cc-9836-cf77704c49a7', 'display': 'VHM SERVICES INC'}} ... 1265 parameters truncated ... 'urn:uuid:b7f285d1-2660-a6bd-550d-e2fe47d70562', 'Immunization', 'b7f285d1-2660-a6bd-550d-e2fe47d70562', '', {'resourceType': 'Immunization', 'id': 'b7f285d1-2660-a6bd-550d-e2fe47d70562', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefinition/u ... (497 characters truncated) ... : {'reference': 'Location?identifier=https://github.com/synthetichealth/synthea|22cc43f7-8383-3158-8247-b5e37eefe30d', 'display': 'VHM SERVICES INC'}}, 'urn:uuid:acd5c5b8-c11e-4ec4-0f28-79db43c77d48', 'DiagnosticReport', 'acd5c5b8-c11e-4ec4-0f28-79db43c77d48', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'DiagnosticReport', 'id': 'acd5c5b8-c11e-4ec4-0f28-79db43c77d48', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefiniti ... (1306 characters truncated) ... 3a-2780-4042-f9bb-b01309454602', 'display': 'Chloride'}, {'reference': 'urn:uuid:71275e44-d982-4c1e-6658-2ecb700196e2', 'display': 'Carbon Dioxide'}]}, 'urn:uuid:6c7571a4-fb06-694d-ea6e-97372a033f76', 'DiagnosticReport', '6c7571a4-fb06-694d-ea6e-97372a033f76', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'DiagnosticReport', 'id': '6c7571a4-fb06-694d-ea6e-97372a033f76', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefiniti ... (997 characters truncated) ... Density Lipoprotein Cholesterol'}, {'reference': 'urn:uuid:cf0125e4-1f40-ecf7-aabb-97013bd6a249', 'display': 'High Density Lipoprotein Cholesterol'}]}, 'urn:uuid:d44d0f90-a70e-cb10-0a5f-8f655e0935b3', 'DiagnosticReport', 'd44d0f90-a70e-cb10-0a5f-8f655e0935b3', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'DiagnosticReport', 'id': 'd44d0f90-a70e-cb10-0a5f-8f655e0935b3', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefiniti ... (811 characters truncated) ... t': [{'reference': 'urn:uuid:5a3213ed-4fe0-6cfa-7ce8-e332ac4dc7bf', 'display': 'Patient Health Questionnaire 2 item (PHQ-2) total score [Reported]'}]}, 'urn:uuid:e2d70203-53ee-9849-8fb8-7d1cecf5071c', 'DiagnosticReport', 'e2d70203-53ee-9849-8fb8-7d1cecf5071c', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'DiagnosticReport', 'id': 'e2d70203-53ee-9849-8fb8-7d1cecf5071c', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefiniti ... (786 characters truncated) ... 9a7', 'display': 'VHM SERVICES INC'}], 'result': [{'reference': 'urn:uuid:3aae8ef7-73dd-0316-e968-5f225d1c304e', 'display': 'Total score [AUDIT-C]'}]}, 'urn:uuid:587be04a-3203-f1a8-dd92-1d5c3cf49035', 'DiagnosticReport', '587be04a-3203-f1a8-dd92-1d5c3cf49035', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'DiagnosticReport', 'id': '587be04a-3203-f1a8-dd92-1d5c3cf49035', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefiniti ... (2575 characters truncated) ... N0YW5jZSB1c2UgKHByb2NlZHVyZSkKLSBhc3Nlc3NtZW50IHVzaW5nIGFsY29ob2wgdXNlIGRpc29yZGVycyBpZGVudGlmaWNhdGlvbiB0ZXN0IC0gY29uc3VtcHRpb24gKHByb2NlZHVyZSkK'}]}, 'urn:uuid:00e88f08-b8b2-fc8b-0c26-05873b144840', 'DocumentReference', '00e88f08-b8b2-fc8b-0c26-05873b144840', 'urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', {'resourceType': 'DocumentReference', 'id': '00e88f08-b8b2-fc8b-0c26-05873b144840', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefinit ... (3033 characters truncated) ... 'reference': 'urn:uuid:63ab952d-0cbf-4389-a550-cc400085c4d4'}], 'period': {'start': '2021-05-27T03:00:07+01:00', 'end': '2021-05-27T03:15:07+01:00'}}}, 'urn:uuid:178edcf7-7ab0-ba6e-c12e-97d5b5dc27d1', 'Claim', '178edcf7-7ab0-ba6e-c12e-97d5b5dc27d1', '', {'resourceType': 'Claim', 'id': '178edcf7-7ab0-ba6e-c12e-97d5b5dc27d1', 'status': 'active', 'type': {'coding': [{'system': 'http://terminology.hl7.org ... (4723 characters truncated) ... ntification Test - Consumption (procedure)'}, 'net': {'value': 516.65, 'currency': 'USD'}}], 'total': {'value': 786.3299999999999, 'currency': 'USD'}}, 'urn:uuid:bc0ca7de-f8df-63a1-a727-59ef08bbe4a2', 'ExplanationOfBenefit', 'bc0ca7de-f8df-63a1-a727-59ef08bbe4a2', '', {'resourceType': 'ExplanationOfBenefit', 'id': 'bc0ca7de-f8df-63a1-a727-59ef08bbe4a2', 'contained': [{'resourceType': 'ServiceRequest', 'id': 'referra ... (19413 characters truncated) ... ext': 'Submitted Amount'}, 'amount': {'value': 786.3299999999999, 'currency': 'USD'}}], 'payment': {'amount': {'value': 2179.016, 'currency': 'USD'}}}, 'urn:uuid:2c71cded-ab89-393c-58e9-91d17cde96a3', 'Provenance', '2c71cded-ab89-393c-58e9-91d17cde96a3', '', {'resourceType': 'Provenance', 'id': '2c71cded-ab89-393c-58e9-91d17cde96a3', 'meta': {'profile': ['http://hl7.org/fhir/us/core/StructureDefinition/us- ... (18347 characters truncated) ... ference': 'Organization?identifier=https://github.com/synthetichealth/synthea|c1bf7bfd-7287-37cc-9836-cf77704c49a7', 'display': 'VHM SERVICES INC'}}]})]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json']\n",
    "\n",
    "patient_data_list = []\n",
    "other_resources_list = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        resource = entry['resource']\n",
    "        if resource['resourceType'] == 'Patient': \n",
    "            patient_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': resource['resourceType'],\n",
    "                'id': resource['id'],\n",
    "                'gender': resource.get('gender', ''),  \n",
    "                'birthDate': resource.get('birthDate', ''), \n",
    "                'deceasedDateTime': resource.get('deceasedDateTime', ''),\n",
    "                'maritalStatus': resource.get('maritalStatus', {}).get('text', ''),\n",
    "                'city': resource['address'][0].get('city', '') if 'address' in resource and resource['address'] else '', \n",
    "                'state': resource['address'][0].get('state', '') if 'address' in resource and resource['address'] else '',\n",
    "                'country': resource['address'][0].get('country', '') if 'address' in resource and resource['address'] else ''\n",
    "            }\n",
    "            patient_data_list.append(patient_data)\n",
    "        else: \n",
    "            resource_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': resource['resourceType'],\n",
    "                'id': resource['id'],\n",
    "                'patientId': resource.get('subject', {}).get('reference', '').split('/')[-1],  # Assuming 'subject' reference links to patient ID\n",
    "                'details': resource  \n",
    "            }\n",
    "            other_resources_list.append(resource_data)\n",
    "\n",
    "patient_df = pd.DataFrame(patient_data_list)\n",
    "resources_df = pd.DataFrame(other_resources_list)\n",
    "patient_df.to_csv('patient_information.csv', index=False)\n",
    "resources_df.to_csv('resources_information.csv', index=False)\n",
    "\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "\n",
    "connection_string = f'mssql+pyodbc://{server}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "try:\n",
    "    patient_table_name = 'PatientInformation'\n",
    "    resources_table_name = 'ResourcesInformation'\n",
    "\n",
    "    patient_df.to_sql(patient_table_name, engine, if_exists='replace', index=False)\n",
    "    resources_df.to_sql(resources_table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "    print(f\"Data has been successfully saved to the SQL Server database '{database}', tables '{patient_table_name}' and '{resources_table_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to SQL Server: {e}\")\n",
    "finally:\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aca5373a-cb0a-4c47-83cd-8579547c98f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to the SQL Server database 'EMIS', tables 'PatientInformation' and 'ResourcesInformation'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json']\n",
    "patient_data_list = []\n",
    "other_resources_list = []\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        resource = entry['resource']\n",
    "        if resource['resourceType'] == 'Patient': \n",
    "            patient_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': resource['resourceType'],\n",
    "                'id': resource['id'],\n",
    "                'gender': resource.get('gender', ''),  \n",
    "                'birthDate': resource.get('birthDate', ''),  \n",
    "                'deceasedDateTime': resource.get('deceasedDateTime', ''),\n",
    "                'maritalStatus': resource.get('maritalStatus', {}).get('text', ''),\n",
    "                'city': resource['address'][0].get('city', '') if 'address' in resource and resource['address'] else '',  \n",
    "                'state': resource['address'][0].get('state', '') if 'address' in resource and resource['address'] else '',\n",
    "                'country': resource['address'][0].get('country', '') if 'address' in resource and resource['address'] else ''\n",
    "            }\n",
    "            patient_data_list.append(patient_data)\n",
    "        else:  \n",
    "            resource_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': resource['resourceType'],\n",
    "                'id': resource['id'],\n",
    "                'patientId': resource.get('subject', {}).get('reference', '').split('/')[-1],  \n",
    "                'details': json.dumps(resource)  \n",
    "            }\n",
    "            other_resources_list.append(resource_data)\n",
    "patient_df = pd.DataFrame(patient_data_list)\n",
    "resources_df = pd.DataFrame(other_resources_list)\n",
    "patient_df.to_csv('patient_information.csv', index=False)\n",
    "resources_df.to_csv('resources_information.csv', index=False)\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "connection_string = f'mssql+pyodbc://{server}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
    "engine = create_engine(connection_string)\n",
    "try:\n",
    "    patient_table_name = 'PatientInformation'\n",
    "    resources_table_name = 'ResourcesInformation'\n",
    "\n",
    "    patient_df.to_sql(patient_table_name, engine, if_exists='replace', index=False)\n",
    "    resources_df.to_sql(resources_table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "    print(f\"Data has been successfully saved to the SQL Server database '{database}', tables '{patient_table_name}' and '{resources_table_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to SQL Server: {e}\")\n",
    "finally:\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bcd9b53-06c6-45bb-a902-b3f75de1b0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully appended to the SQL Server database 'EMIS', table 'PatientInformation'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = [\n",
    "    'Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json',\n",
    "    'Aaron697_Jerde200_6fa23508-960e-ff22-c3d0-0519a036543b.json'\n",
    "]\n",
    "def fetch_and_process_json(json_file):\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    patient_data_list = []\n",
    "\n",
    "    for entry in data['entry']:\n",
    "        patient = entry['resource']\n",
    "        if patient['resourceType'] == 'Patient': \n",
    "            patient_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': patient['resourceType'],\n",
    "                'id': patient['id'],\n",
    "                'gender': patient.get('gender', ''),  \n",
    "                'birthDate': patient.get('birthDate', ''),  \n",
    "                'deceasedDateTime': patient.get('deceasedDateTime', ''),\n",
    "                'maritalStatus': patient.get('maritalStatus', {}).get('text', ''),\n",
    "                'city': patient['address'][0].get('city', '') if 'address' in patient and patient['address'] else '', \n",
    "                'state': patient['address'][0].get('state', '') if 'address' in patient and patient['address'] else '',\n",
    "                'country': patient['address'][0].get('country', '') if 'address' in patient and patient['address'] else ''\n",
    "            }\n",
    "            patient_data_list.append(patient_data)\n",
    "    \n",
    "    return patient_data_list\n",
    "all_patient_data = []\n",
    "for json_file in json_files:\n",
    "    all_patient_data.extend(fetch_and_process_json(json_file))\n",
    "\n",
    "df = pd.DataFrame(all_patient_data)\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "connection_string = f'mssql+pyodbc://{server}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "try:\n",
    "    table_name = 'PatientInformation'\n",
    "    df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "    print(f\"Data has been successfully appended to the SQL Server database '{database}', table '{table_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to SQL Server: {e}\")\n",
    "finally:\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54c68ecb-1cb9-45d0-b205-110bcb4d4c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving data to SQL Server: (pyodbc.ProgrammingError) ('42S22', \"[42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'name'. (207) (SQLExecDirectW); [42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Statement(s) could not be prepared. (8180)\")\n",
      "[SQL: INSERT INTO [PatientInformation] ([fullUrl], [resourceType], id, name, gender, [birthDate], [deceasedDateTime], [maritalStatus], city, state, country) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]\n",
      "[parameters: ('urn:uuid:e5b7d947-da2b-9cf4-12a0-9eb2cf735330', 'Patient', 'e5b7d947-da2b-9cf4-12a0-9eb2cf735330', 'Chase54 Crooks415', 'male', '1977-05-12', '', 'M', 'Franklin', 'MA', 'US')]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine, text\n",
    "import json\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json']\n",
    "patient_data_list = []\n",
    "other_resources_list = []\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        resource = entry['resource']\n",
    "        if resource['resourceType'] == 'Patient':\n",
    "            name = ''\n",
    "            if 'name' in resource and resource['name']:\n",
    "                name_parts = []\n",
    "                for part in resource['name'][0]['given']:\n",
    "                    name_parts.append(part)\n",
    "                name_parts.append(resource['name'][0]['family'])\n",
    "                name = ' '.join(name_parts)\n",
    "                \n",
    "            patient_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': resource['resourceType'],\n",
    "                'id': resource['id'],\n",
    "                'name': name,  # Adding the name field\n",
    "                'gender': resource.get('gender', ''),\n",
    "                'birthDate': resource.get('birthDate', ''),\n",
    "                'deceasedDateTime': resource.get('deceasedDateTime', ''),\n",
    "                'maritalStatus': resource.get('maritalStatus', {}).get('text', ''),\n",
    "                'city': resource['address'][0].get('city', '') if 'address' in resource and resource['address'] else '',\n",
    "                'state': resource['address'][0].get('state', '') if 'address' in resource and resource['address'] else '',\n",
    "                'country': resource['address'][0].get('country', '') if 'address' in resource and resource['address'] else ''\n",
    "            }\n",
    "            patient_data_list.append(patient_data)\n",
    "        else:\n",
    "            # Collecting other resource data\n",
    "            resource_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': resource['resourceType'],\n",
    "                'id': resource['id'],\n",
    "                'patientId': resource.get('subject', {}).get('reference', '').split('/')[-1],\n",
    "                'details': json.dumps(resource)\n",
    "            }\n",
    "            other_resources_list.append(resource_data)\n",
    "\n",
    "patient_df = pd.DataFrame(patient_data_list)\n",
    "resources_df = pd.DataFrame(other_resources_list)\n",
    "\n",
    "patient_df.to_csv('patient_information.csv', index=False)\n",
    "resources_df.to_csv('resources_information.csv', index=False)\n",
    "\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "connection_string = f'mssql+pyodbc://{server}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "def delete_existing_records(engine, patient_id):\n",
    "    with engine.connect() as connection:\n",
    "        delete_query = text(\"DELETE FROM PatientInformation WHERE id = :id\")\n",
    "        connection.execute(delete_query, {'id': patient_id})\n",
    "\n",
    "\n",
    "try:\n",
    "    patient_table_name = 'PatientInformation'\n",
    "    resources_table_name = 'ResourcesInformation'\n",
    "\n",
    "    for index, row in patient_df.iterrows():\n",
    "        delete_existing_records(engine, row['id'])\n",
    "        row.to_frame().T.to_sql(patient_table_name, engine, if_exists='append', index=False)\n",
    "\n",
    "    for index, row in resources_df.iterrows():\n",
    "        row.to_frame().T.to_sql(resources_table_name, engine, if_exists='append', index=False)\n",
    "\n",
    "    print(f\"Data has been successfully saved to the SQL Server database '{database}', tables '{patient_table_name}' and '{resources_table_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to SQL Server: {e}\")\n",
    "finally:\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7665e9b-8810-40a5-b47a-f2cc8c863a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to the SQL Server database 'EMIS', tables 'PatientInformation' and 'ResourcesInformation'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine, text\n",
    "import json\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json','Aaron697_Dickens475_8c95253e-8ee8-9ae8-6d40-021d702dc78e.json','Beth967_Hansen121_4e343b0a-8698-b6dd-64c6-c2d2d0959e6e.json']\n",
    "patient_data_list = []\n",
    "other_resources_list = []\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        resource = entry['resource']\n",
    "        if resource['resourceType'] == 'Patient':\n",
    "            name = ''\n",
    "            if 'name' in resource and resource['name']:\n",
    "                name_parts = []\n",
    "                for part in resource['name'][0]['given']:\n",
    "                    name_parts.append(part)\n",
    "                name_parts.append(resource['name'][0]['family'])\n",
    "                name = ' '.join(name_parts)\n",
    "                \n",
    "            patient_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': resource['resourceType'],\n",
    "                'id': resource['id'],\n",
    "                'name': name,  # Adding the name field\n",
    "                'gender': resource.get('gender', ''),\n",
    "                'birthDate': resource.get('birthDate', ''),\n",
    "                'deceasedDateTime': resource.get('deceasedDateTime', ''),\n",
    "                'maritalStatus': resource.get('maritalStatus', {}).get('text', ''),\n",
    "                'city': resource['address'][0].get('city', '') if 'address' in resource and resource['address'] else '',\n",
    "                'state': resource['address'][0].get('state', '') if 'address' in resource and resource['address'] else '',\n",
    "                'country': resource['address'][0].get('country', '') if 'address' in resource and resource['address'] else ''\n",
    "            }\n",
    "            patient_data_list.append(patient_data)\n",
    "        else:\n",
    "            # Collecting other resource data\n",
    "            resource_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': resource['resourceType'],\n",
    "                'id': resource['id'],\n",
    "                'patientId': resource.get('subject', {}).get('reference', '').split('/')[-1],\n",
    "                'details': json.dumps(resource)\n",
    "            }\n",
    "            other_resources_list.append(resource_data)\n",
    "\n",
    "patient_df = pd.DataFrame(patient_data_list)\n",
    "resources_df = pd.DataFrame(other_resources_list)\n",
    "\n",
    "patient_df.to_csv('patient_information.csv', index=False)\n",
    "resources_df.to_csv('resources_information.csv', index=False)\n",
    "\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "connection_string = f'mssql+pyodbc://{server}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "def delete_existing_records(engine, patient_id):\n",
    "    with engine.connect() as connection:\n",
    "        delete_query = text(\"DELETE FROM PatientInformation WHERE id = :id\")\n",
    "        connection.execute(delete_query, {'id': patient_id})\n",
    "\n",
    "\n",
    "try:\n",
    "    patient_table_name = 'PatientInformation'\n",
    "    resources_table_name = 'ResourcesInformation'\n",
    "\n",
    "    for index, row in patient_df.iterrows():\n",
    "        delete_existing_records(engine, row['id'])\n",
    "        row.to_frame().T.to_sql(patient_table_name, engine, if_exists='append', index=False)\n",
    "\n",
    "    for index, row in resources_df.iterrows():\n",
    "        row.to_frame().T.to_sql(resources_table_name, engine, if_exists='append', index=False)\n",
    "\n",
    "    print(f\"Data has been successfully saved to the SQL Server database '{database}', tables '{patient_table_name}' and '{resources_table_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to SQL Server: {e}\")\n",
    "finally:\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2821dea6-8246-48d3-80b1-4904f57ff747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error deleting existing records: mapping or list expected for parameters\n",
      "Data has been successfully saved to the SQL Server database 'EMIS', tables 'PatientInformation' and 'ResourcesInformation'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine, text\n",
    "import json\n",
    "\n",
    "def delete_existing_records(engine, patient_ids):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            placeholders = ', '.join(['?' for _ in patient_ids])\n",
    "            delete_query = text(f\"DELETE FROM PatientInformation WHERE id IN ({placeholders})\")\n",
    "            connection.execute(delete_query, patient_ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting existing records: {e}\")\n",
    "\n",
    "base_url = 'https://raw.githubusercontent.com/Shyamalasridar/exa-data-eng-assessment/main/data/'\n",
    "json_files = ['Chase54_Crooks415_e5b7d947-da2b-9cf4-12a0-9eb2cf735330.json','Aaron697_Dickens475_8c95253e-8ee8-9ae8-6d40-021d702dc78e.json','Beth967_Hansen121_4e343b0a-8698-b6dd-64c6-c2d2d0959e6e.json']\n",
    "\n",
    "patient_data_list = []\n",
    "other_resources_list = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    response = requests.get(base_url + json_file)\n",
    "    data = response.json()\n",
    "    for entry in data['entry']:\n",
    "        resource = entry['resource']\n",
    "        if resource['resourceType'] == 'Patient':\n",
    "            name = ''\n",
    "            if 'name' in resource and resource['name']:\n",
    "                name_parts = []\n",
    "                for part in resource['name'][0]['given']:\n",
    "                    name_parts.append(part)\n",
    "                name_parts.append(resource['name'][0]['family'])\n",
    "                name = ' '.join(name_parts)\n",
    "                \n",
    "            patient_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': resource['resourceType'],\n",
    "                'id': resource['id'],\n",
    "                'name': name,\n",
    "                'gender': resource.get('gender', ''),\n",
    "                'birthDate': resource.get('birthDate', ''),\n",
    "                'deceasedDateTime': resource.get('deceasedDateTime', ''),\n",
    "                'maritalStatus': resource.get('maritalStatus', {}).get('text', ''),\n",
    "                'city': resource['address'][0].get('city', '') if 'address' in resource and resource['address'] else '',\n",
    "                'state': resource['address'][0].get('state', '') if 'address' in resource and resource['address'] else '',\n",
    "                'country': resource['address'][0].get('country', '') if 'address' in resource and resource['address'] else ''\n",
    "            }\n",
    "            patient_data_list.append(patient_data)\n",
    "        else:\n",
    "            resource_data = {\n",
    "                'fullUrl': entry['fullUrl'],\n",
    "                'resourceType': resource['resourceType'],\n",
    "                'id': resource['id'],\n",
    "                'patientId': resource.get('subject', {}).get('reference', '').split('/')[-1],\n",
    "                'details': json.dumps(resource)\n",
    "            }\n",
    "            other_resources_list.append(resource_data)\n",
    "\n",
    "patient_df = pd.DataFrame(patient_data_list)\n",
    "resources_df = pd.DataFrame(other_resources_list)\n",
    "\n",
    "patient_df.to_csv('patient_information.csv', index=False)\n",
    "resources_df.to_csv('resources_information.csv', index=False)\n",
    "server = 'AMSAS'\n",
    "database = 'EMIS'\n",
    "connection_string = f'mssql+pyodbc://{server}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
    "\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "try:\n",
    "    patient_table_name = 'PatientInformation'\n",
    "    resources_table_name = 'ResourcesInformation'\n",
    "\n",
    "    patient_ids = patient_df['id'].unique()\n",
    "\n",
    "    delete_existing_records(engine, patient_ids)\n",
    "\n",
    "    patient_df.to_sql(patient_table_name, engine, if_exists='append', index=False)\n",
    "    resources_df.to_sql(resources_table_name, engine, if_exists='append', index=False)\n",
    "\n",
    "    print(f\"Data has been successfully saved to the SQL Server database '{database}', tables '{patient_table_name}' and '{resources_table_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to SQL Server: {e}\")\n",
    "finally:\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c10d2-5290-4393-93cf-846f96044b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
